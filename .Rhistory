?fleishman.coef
fleishman.coef(1, G[3], G[4])
find_constants("Fleishman", G[3], G[4])
find_constants("Fleishman", round(G[3],10), round(G[4],10))
Fleishman.coef.NN(G[3],G[4])
pdf_check(Fleishman.coef.NN(G[3],G[4])[1,], "Fleishman")
pdf_check(find_constants("Fleishman", G[3], G[4])$constants, "Fleishman")
F <- calc_theory("Fisk", c(1,4)); F
?calc_theory
?dexppois
F <- calc_theory("Exp-Poisson", c(2,0.5)); F
F2 <- find_constants("Polynomial", F[3], F[4], F[5], F[6])
F2
F2 <- find_constants("Polynomial", F[3], F[4], F[5], F[6], Six = seq(0.01,6,0.01))
F2
F3 <- find_constants("Fleishman", F[3], F[4])
F3
fleishman.coef(1, F[3],F[4])
Fleishman.coef.NN(F[3],F[4])
Dist <- c("Rayleigh", "Exp-Poisson", "Beta", "Chisq")
Params <- list(c(2, 4), c(2, 0.5), c(4, 1.5), 3)
# list of sixth cumulant corrections to correct for invalid power method pdfs
Six <- list(seq(0.12, 5, 0.01), seq(0.04, 6, 0.01), 0.03, NULL)
# Calculate standardized cumulants
Stcum1 <- calc_theory(Dist[1], Params[[1]])
Stcum2 <- calc_theory(Dist[2], Params[[2]])
Stcum3 <- calc_theory(Dist[3], Params[[3]])
Stcum4 <- calc_theory(Dist[4], Params[[4]])
Stcum <- cbind(Stcum1, Stcum2, Stcum3, Stcum4)
colnames(Stcum) <- Dist
rownames(Stcum) <- c("mean", "sd", "skew", "skurtosis", "fifth", "sixth")
Stcum
# Ordinal Distributions
marginal <- list(c(0.3, 0.6), c(0.2, 0.5, 0.75))
# Poisson Distributions
lam <- c(0.3, 0.4)
ncat <- length(marginal)
ncont <- ncol(Stcum)
npois <- length(lam)
# cumulative probability truncation values for use in method 2 (rcorrvar2)
pois_eps <- rep(0.0001, npois)
set.seed(seed)
Rey <- diag(1, nrow = (ncat + ncont + npois))
for (i in 1:nrow(Rey)) {
for (j in 1:ncol(Rey)) {
if (i > j) Rey[i, j] <- runif(1, -0.4, 0.4)
Rey[j, i] <- Rey[i, j]
}
}
# Test for positive-definiteness
if(min(eigen(Rey, symmetric = TRUE)$values) < 0) {
Rey <- as.matrix(nearPD(Rey, corr = T, keepDiag = T)$mat)
}
fifth <- rcorrvar2(n = n, k_cont = ncont, k_cat = ncat, k_pois = npois,
method = "Polynomial", means = Stcum[1, ],
vars = Stcum[2, ]^2, skews = Stcum[3, ],
skurts = Stcum[4, ], fifths = Stcum[5, ],
sixths = Stcum[6, ], Six = Six, marginal = marginal,
lam = lam, pois_eps = pois_eps,
rho = Rey, seed = seed)
fifth$maxerr
fifth$valid.pdf
fifth$sixth_correction
fifth$summary_targetcont
fifth$summary_cont
Fleishman.coef.NN(Stcum[3,],Stcum[4,])
N <- calc_theory("Nakagami", c(1, 2))
N
N <- calc_theory("Nakagami", c(1, 3)); N
N <- calc_theory("Nakagami", c(1, 0.5)); N
find_constants("Polynomial", N[3], N[4], N[5], N[6], Six = seq(0.01, 6, 0.01))
find_constants("Polynomial", N[3], N[4], N[5], N[6], Six = seq(0.01, 6, 0.01), seed = 104)
f5 <- "C:\\Users\\Allison\\Desktop\\Dissertation\\PBON_new2\\"
Dist <- c("Rayleigh", "Exp-Poisson", "Beta", "Chisq")
Sum_cont <- list()
for (i in 1:20) {
Sum_cont <- append(Sum_cont, readRDS(paste(f5, "Sum_cont_", i, ".rds", sep = "")))
}
Sum_cont2 <- list()
for (i in 1:20) {
Sum_cont2 <- append(Sum_cont2, readRDS(paste(f5, "Sum_cont2_", i, ".rds", sep = "")))
}
# Continuous variables
qsumPBON <- quantile_summary(Sum_cont, digits = 2)[, c(3:4, 8:11)]
rownames(qsumPBON) <- Dist
qsum2 <- quantile_summary(Sum_cont2, digits = 2)[, c(3:4, 8:11)]
Dist <- c("Rayleigh", "Exp-Poisson", "Beta", "Chisq")
Sum_cont <- list()
for (i in 1:20) {
Sum_cont <- append(Sum_cont, readRDS(paste(f5, "Sum_cont_PBON_", i, ".rds", sep = "")))
}
Sum_cont2 <- list()
for (i in 1:20) {
Sum_cont2 <- append(Sum_cont2, readRDS(paste(f5, "Sum_cont2_", i, ".rds", sep = "")))
}
# Continuous variables
qsumPBON <- quantile_summary(Sum_cont, digits = 2)[, c(3:4, 8:11)]
rownames(qsumPBON) <- Dist
qsum2 <- quantile_summary(Sum_cont2, digits = 2)[, c(3:4, 8:11)]
rownames(qsum2) <- Dist
Dist <- c("Rayleigh", "Exp-Poisson", "Beta", "Chisq")
Params <- list(c(2, 4), c(2, 0.5), c(4, 1.5), 3)
# list of sixth cumulant corrections to correct for invalid power method pdfs
Six <- list(seq(0.12, 5, 0.01), seq(0.01, 6, 0.01), 0.03, NULL)
# Calculate standardized cumulants
Stcum1 <- calc_theory(Dist[1], Params[[1]])
Stcum2 <- calc_theory(Dist[2], Params[[2]])
Stcum3 <- calc_theory(Dist[3], Params[[3]])
Stcum4 <- calc_theory(Dist[4], Params[[4]])
Stcum <- cbind(Stcum1, Stcum2, Stcum3, Stcum4)
colnames(Stcum) <- Dist
rownames(Stcum) <- c("mean", "sd", "skew", "skurtosis", "fifth", "sixth")
t(Stcum)
qsum2
qsumPBON
65594.31/(60^2)
f6 <- "C:\\Users\\Allison\\Desktop\\Dissertation\\PBON_new3\\"
PBONtime <- readRDS(paste(f6, "Total_Time_PBON_3.rds", sep = ""))
Sum_cont <- readRDS(paste(f6, "Sum_cont_PBON_3.rds", sep = ""))
# METHOD 2 Total time
# 65594.31/(60^2)
# [1] 18.22064
Sum_cont <- readRDS(paste(f6, "Sum_cont2.rds", sep = ""))
Dist <- c("Rayleigh", "Pareto", "Beta", "Chisq")
Params <- list(c(2, 4), c(10, 20), c(4, 1.5), 3)
# list of sixth cumulant corrections to correct for invalid power method pdfs
Six <- list(seq(0.12, 5, 0.01), seq(0.01, 6, 0.01), 0.03, NULL)
# Calculate standardized cumulants
Stcum1 <- calc_theory(Dist[1], Params[[1]])
Stcum2 <- calc_theory(Dist[2], Params[[2]])
Stcum3 <- calc_theory(Dist[3], Params[[3]])
Stcum4 <- calc_theory(Dist[4], Params[[4]])
Stcum <- cbind(Stcum1, Stcum2, Stcum3, Stcum4)
colnames(Stcum) <- Dist
rownames(Stcum) <- c("mean", "sd", "skew", "skurtosis", "fifth", "sixth")
qsumPBON <- quantile_summary(Sum_cont, digits = 2)[, c(3:4, 8:11)]
rownames(qsumPBON) <- Dist
qsum2 <- quantile_summary(Sum_cont2, digits = 2)[, c(3:4, 8:11)]
rownames(qsum2) <- Dist
t(Stcum)
qsumPBON
PBONtime <- readRDS(paste(f6, "Total_Time_PBON_3.rds", sep = ""))
Sum_cont <- readRDS(paste(f6, "Sum_cont_PBON_3.rds", sep = ""))
# METHOD 2 Total time
# 65594.31/(60^2)
# [1] 18.22064
Sum_cont2 <- readRDS(paste(f6, "Sum_cont2.rds", sep = ""))
qsumPBON <- quantile_summary(Sum_cont, digits = 2)[, c(3:4, 8:11)]
rownames(qsumPBON) <- Dist
qsum2 <- quantile_summary(Sum_cont2, digits = 2)[, c(3:4, 8:11)]
rownames(qsum2) <- Dist
t(Stcum)
qsumPBON
qsum2
65594.31/(60^2)
Fleishman.coef.NN
G <- calc_theory("Gumbel", c(3,5)); G
find_constants("Fleishman", G[3], G[4])
G <- calc_theory("Gompertz", c(3,5)); G
find_constants("Fleishman", G[3], G[4])
find_constants("Polynomial", G[3], G[4], G[5], G[6], Six = seq(0.01, 6, 0.01))
133880.6/(60^2)
f1 <- "C:\\Users\\Allison\\Desktop\\Dissertation\\Method1v2\\"
f2 <- "C:\\Users\\Allison\\Desktop\\Dissertation\\M1v2\\"
f3 <- "C:\\Users\\Allison\\Desktop\\Dissertation\\M1v2neg\\"
f4 <- "C:\\Users\\Allison\\Desktop\\Dissertation\\PBON_new\\"
f5 <- "C:\\Users\\Allison\\Desktop\\Dissertation\\PBON_new2\\"
f6 <- "C:\\Users\\Allison\\Desktop\\Dissertation\\PBON_new3\\"
options(scipen = 999)
quantile_summary <- function(x_list, q1 = 0.25, q2 = 0.75, digits = 3) {
as.data.frame(rbindlist(lapply(x_list, data.table,
keep.rownames = TRUE))[,
lapply(.SD,
function(x) paste(round(median(x, na.rm = TRUE), digits), " (",
round(quantile(x, probs = q1, na.rm = TRUE), digits), ", ",
round(quantile(x, probs = q2, na.rm = TRUE), digits), ")",
sep = "")), by = rn][, rn := NULL])
}
seed <- 1234
# repetitions
rep <- 10000
# sample size
n <- 10000
# Continuous Distributions
Dist <- c("Triangular", "Beta")
Params <- list(c(1, 6, 4), c(6, 3))
# Calculate standardized cumulants
Stcum1 <- calc_theory(Dist[1], Params[[1]])
Stcum2 <- calc_theory(Dist[2], Params[[2]])
Stcum <- cbind(Stcum1, Stcum2)
colnames(Stcum) <- Dist
rownames(Stcum) <- c("mean", "sd", "skew", "skurtosis", "fifth", "sixth")
# list of sixth cumulant corrections to correct for invalid power method pdfs
Six <- list(seq(0.52, 0.8, 0.01), seq(0.01, 1.07, 0.01))
# no ordinal distributions will be simulated
marginal <- NULL
# Poisson Distributions
lam <- c(0.1, 0.3, 10, 20)
# Negative Binomial Distributions
size <- c(2, 5, 50, 25)
prob <- c(0.75, 0.9, 0.7, 0.5)
ncat <- length(marginal)
ncont <- ncol(Stcum)
npois <- length(lam)
nnb <- length(size)
Rey <- matrix(0.4, 10, 10)
diag(Rey) <- 1
# Continuous variables
Sum_cont1 <- list()
for (i in 1:20) {
Sum_cont1 <- append(Sum_cont1, readRDS(paste(f2, "Sum_cont1_", i, ".rds", sep = "")))
}
qsum1 <- quantile_summary(Sum_cont1)
# display summary of simulated continuous variables using rcorrvar with target
# correlation matrix Rey (positive correlation case)
Cont_sum <- as.data.frame(cbind(round(Stcum[, "Triangular"], 3),
t(qsum1[1, c(3:4, 8:11)]),
round(Stcum[, "Beta"], 3),
t(qsum1[2, c(3:4, 8:11)])))
Cont_sum
print(xtable(Cont_sum,
caption = "Summary of continuous variables for example 1.",
label = "table_sum_cont1"))
f7 <- "C:\\Users\\Allison\\Desktop\\Dissertation\\Ex1_Beta_Naka\\"
f8 <- "C:\\Users\\Allison\\Desktop\\Dissertation\\Ex1_Beta_Naka\\Neg\\"
colnames(Cont_sum) <- c("Triangular", "Method 1", "Beta", "Method 1")
rownames(Cont_sum) <- c("Mean", "SD", "Skew", "Skurtosis", "Fifth", "Sixth")
Cont_sum
print(xtable(Cont_sum,
caption = "Target distributions and median (IQR) of continuous
variables for example 1: Simulations with rcorrvar and target
correlation Rey.",
label = "table_sum_cont1"))
library("data.table")
library("xtable")
library("Matrix")
library("ggplot2")
library("gridExtra")
# graph last set of simulated continuous variables using rcorrvar with target
# correlation matrix Rey (positive correlation case) -- legends were removed
Cont1 <- read.table(paste(f2, "Cont1_20.txt", sep = ""), header = F)
# graph last set of simulated continuous variables using rcorrvar with target
# correlation matrix Rey (positive correlation case) -- legends were removed
Cont1 <- read.table(paste(f2, "Cont1_20.txt", sep = ""))
head(Cont1)
# graph last set of simulated continuous variables using rcorrvar with target
# correlation matrix Rey (positive correlation case) -- legends were removed
Cont1 <- read.table(paste(f2, "Cont1_20.txt", sep = ""), header = TRUE)
head(Cont1)
Tri_pdf <- plot_sim_pdf_theory(sim_y = Cont1[, 1],
Dist = "Triangular",
params = c(1, 6, 4),
title = "Triangular(1, 6, 4) distribution",
legend.position = "none")
Beta_pdf <- plot_sim_pdf_theory(sim_y = Cont1[, 2],
Dist = "Beta",
params = c(6, 3),
title = "Beta(6, 3) distribution",
legend.position = "none")
grid.arrange(Tri_pdf, Beta_pdf, nrow = 1)
Sum_cont1 <- list()
Valid.pdf1 <- list()
Sixcorr1 <- list()
for (i in 1:20) {
Sum_cont1 <- append(Sum_cont1, readRDS(paste(f2, "Sum_cont1_", i, ".rds", sep = "")))
Valid.pdf1 <- append(Valid.pdf1, readRDS(paste(f2, "Valid.pdf1_", i, ".rds", sep = "")))
Sixcorr1 <- append(Sixcorr1, readRDS(paste(f2, "Sixcorr1_", i, ".rds", sep = "")))
}
valid <- 0
false <- NULL
for (i in 1:length(Valid.pdf1)) {
valid2 <- sum(Valid.pdf1[[i]] == TRUE)
if (valid2 != 2) false <- append(false, i)
valid <- valid + valid2
}
valid
# look at values of sixth cumulant corrections needed to create valid pdf's
# and determine the most frequent sixth cumulant corrections
one <- numeric(rep)
two <- numeric(rep)
for (i in 1:rep) {
one[i] <- Sixcorr1[[i]][1]
two[i] <- Sixcorr1[[i]][2]
}
table(as.factor(one))
table(as.factor(two))
NSum_pois1 <- list()
for (i in 1:20) {
NSum_pois1 <- append(NSum_pois1, readRDS(paste(f3, "NSum_pois1_", i, ".rds", sep = "")))
}
Npois_qsum1 <- quantile_summary(NSum_pois1)
NSum_pois1 <- readRDS(paste(f3, "Sum_pois1.rds", sep = ""))
Npois_qsum1 <- quantile_summary(NSum_pois1)
print(xtable(data.frame(lambda = lam, Npois_qsum1[ c(3, 5, 8:9)]),
caption = "Median (IQR) of Poisson variables for example 1:
Simulations with rcorrvar and target correlation ReyN.",
label = "table_sum_pois1"))
NSum_nb2 <- list()
for (i in 1:20) {
NSum_nb2 <- append(NSum_nb2, readRDS(paste(f3, "Sum_nb2_", i, ".rds", sep = "")))
}
Nnb_qsum2 <- quantile_summary(NSum_nb2)
print(xtable(data.frame(Exp_mean = NSum_nb2[[1]][, "Exp_mean"],
mean = Nnb_qsum2[, "mean"],
Exp_var = NSum_nb2[[1]][, "Exp_var"],
var = Nnb_qsum2[, "var"],
min = Nnb_qsum2[, "min"],
max = Nnb_qsum2[, "max"]),
caption = "Median (IQR) of Negative Binomial variables for
example 1: Simulations with rcorrvar2 and target correlation
ReyN.", label = "table_sum_nb1"))
Corr_error1 <- list()
Corr_error2 <- list()
for (i in 1:20) {
Corr_error1 <- append(Corr_error1, readRDS(paste(f2, "Corr_error1_", i, ".rds", sep = "")))
Corr_error2 <- append(Corr_error2, readRDS(paste(f2, "Corr_error2_", i, ".rds", sep = "")))
}
corr_error1 <- quantile_summary(Corr_error1)
rownames(corr_error1) <- c("C1", "C2", "P1", "P2", "P3", "P4",
"NB1", "NB2", "NB3", "NB4")
colnames(corr_error1) <- rownames(corr_error1)
corr_error2 <- quantile_summary(Corr_error2)
rownames(corr_error2) <- rownames(corr_error1)
colnames(corr_error2) <- rownames(corr_error2)
# display selected correlation errors
print(xtable(corr_error1[c("C1", "C2", "NB1", "NB2", "P1"), c("P1", "P2")]))
print(xtable(corr_error2[c("C1", "C2", "NB1", "NB2", "P1"), c("P1", "P2")],
caption = "Median (IQR) of selected correlation errors with target correlation matrix Rey.",
label = "table_corr_errors1m"))
corr_errors <- matrix(1, nrow = 10, ncol = 10)
for (i in 1:nrow(corr_errors)) {
for (j in 1:ncol(corr_errors)) {
if (i == j) corr_errors[i, j] <- "0"
if (i < j) corr_errors[i, j] <- as.character(corr_error1[i, j])
if (i > j) corr_errors[i, j] <- paste("\\color{blue}",
as.character(corr_error2[i, j]),
sep = " ")
}
}
rownames(corr_errors) <- c("C1", "C2", "P1", "P2", "P3", "P4",
"NB1", "NB2", "NB3", "NB4")
colnames(corr_errors) <- rownames(corr_errors)
# display all correlation errors for table in Supplementary File
print(xtable(corr_errors[, 1:5]),
sanitize.text.function=function(x){x})
print(xtable(corr_errors[, 6:10],
caption = "Median (IQR) of correlation errors using correlation method 1
(in black) and correlation method 2 (in blue) with target correlation matrix Rey.",
label = "table_corr_errors1"),
sanitize.text.function=function(x){x})
ReyN <- read.table(paste(f3, "ReyN.txt", sep = ""))
NCorr1 <- readRDS(paste(f3, "Corr1.rds", sep = ""))
NCorr2 <- list()
for (i in 1:20) {
NCorr2 <- append(NCorr2, readRDS(paste(f3, "Corr2_", i, ".rds", sep = "")))
}
ReyN
NCorr_error1 <- list()
for (i in 1:length(NCorr1)) {
NCorr_error1[[i]] <- as.data.frame(NCorr1[[i]] - ReyN)
NCorr1[[i]] <- as.data.frame(NCorr1[[i]])
rownames(NCorr1[[i]]) <- c(1:nrow(ReyN))
rownames(NCorr_error1[[i]]) <- c(1:nrow(ReyN))
}
NCorr_error2 <- list()
for (i in 1:length(NCorr2)) {
NCorr_error2[[i]] <- as.data.frame(NCorr2[[i]] - ReyN)
NCorr2[[i]] <- as.data.frame(NCorr2[[i]])
rownames(NCorr2[[i]]) <- c(1:nrow(ReyN))
rownames(NCorr_error2[[i]]) <- c(1:nrow(ReyN))
}
Ncorr_error1 <- quantile_summary(NCorr_error1)
rownames(Ncorr_error1) <- c("C1", "C2", "P1", "P2", "P3", "P4",
"NB1", "NB2", "NB3", "NB4")
colnames(Ncorr_error1) <- rownames(Ncorr_error1)
Ncorr_error2 <- quantile_summary(NCorr_error2)
rownames(Ncorr_error2) <- rownames(Ncorr_error1)
colnames(Ncorr_error2) <- rownames(Ncorr_error2)
# display selected correlation errors
print(xtable(Ncorr_error1[c("C1", "C2", "NB1", "NB2", "P1"), c("P1", "P2")]))
print(xtable(Ncorr_error2[c("C1", "C2", "NB1", "NB2", "P1"), c("P1", "P2")],
caption = "Median (IQR) of selected correlation errors with target correlation matrix ReyN.",
label = "table_Ncorr_errors1m"))
Ncorr_errors <- matrix(1, nrow = 10, ncol = 10)
for (i in 1:nrow(Ncorr_errors)) {
for (j in 1:ncol(Ncorr_errors)) {
if (i == j) Ncorr_errors[i, j] <- "0"
if (i < j) Ncorr_errors[i, j] <- as.character(Ncorr_error1[i, j])
if (i > j) Ncorr_errors[i, j] <- paste("\\color{blue}",
as.character(Ncorr_error2[i, j]),
sep = " ")
}
}
rownames(Ncorr_errors) <- c("C1", "C2", "P1", "P2", "P3", "P4",
"NB1", "NB2", "NB3", "NB4")
colnames(Ncorr_errors) <- rownames(Ncorr_errors)
# display all correlation errors for table in Supplementary File
print(xtable(Ncorr_errors[, 1:5]),
sanitize.text.function=function(x){x})
print(xtable(Ncorr_errors[, 6:10],
caption = "Median (IQR) of correlation errors using correlation method 1
(in black) and correlation method 2 (in blue) with target correlation matrix ReyN.",
label = "table_Ncorr_errors1"),
sanitize.text.function=function(x){x})
Dist <- c("Triangular", "Nakagami")
Params <- list(c(1, 6, 4), c(1, 0.5))
# Calculate standardized cumulants
Stcum1 <- calc_theory(Dist[1], Params[[1]])
Stcum2 <- calc_theory(Dist[2], Params[[2]])
Stcum <- cbind(Stcum1, Stcum2)
colnames(Stcum) <- Dist
rownames(Stcum) <- c("mean", "sd", "skew", "skurtosis", "fifth", "sixth")
Sum_cont1 <- list()
Valid.pdf1 <- list()
Sixcorr1 <- list()
for (i in 1:20) {
Sum_cont1 <- append(Sum_cont1, readRDS(paste(f7, "Sum_cont1_", i, ".rds", sep = "")))
Valid.pdf1 <- append(Valid.pdf1, readRDS(paste(f7, "Valid.pdf1_", i, ".rds", sep = "")))
Sixcorr1 <- append(Sixcorr1, readRDS(paste(f7, "Sixcorr1_", i, ".rds", sep = "")))
}
qsum1 <- quantile_summary(Sum_cont1)
valid <- 0
false <- NULL
for (i in 1:length(Valid.pdf1)) {
valid2 <- sum(Valid.pdf1[[i]] == TRUE)
if (valid2 != 2) false <- append(false, i)
valid <- valid + valid2
}
valid
# look at values of sixth cumulant corrections needed to create valid pdf's
# and determine the most frequent sixth cumulant corrections
one <- numeric(rep)
two <- numeric(rep)
for (i in 1:rep) {
one[i] <- Sixcorr1[[i]][1]
two[i] <- Sixcorr1[[i]][2]
}
table(as.factor(one))
table(as.factor(two))
t(Stcum)
qsum1
Corr_error1 <- list()
Corr_error2 <- list()
for (i in 1:20) {
Corr_error1 <- append(Corr_error1, readRDS(paste(f7, "Corr_error1_", i, ".rds", sep = "")))
Corr_error2 <- append(Corr_error2, readRDS(paste(f7, "Corr_error2_", i, ".rds", sep = "")))
}
corr_error1 <- quantile_summary(Corr_error1)
rownames(corr_error1) <- c("C1", "C2", "P1", "P2", "P3", "P4",
"NB1", "NB2", "NB3", "NB4")
colnames(corr_error1) <- rownames(corr_error1)
corr_error2 <- quantile_summary(Corr_error2)
rownames(corr_error2) <- rownames(corr_error1)
colnames(corr_error2) <- rownames(corr_error2)
# display selected correlation errors
print(xtable(corr_error1[c("C1", "C2", "NB1", "NB2", "P1"), c("P1", "P2")]))
print(xtable(corr_error2[c("C1", "C2", "NB1", "NB2", "P1"), c("P1", "P2")],
caption = "Median (IQR) of selected correlation errors with target correlation matrix Rey.",
label = "table_corr_errors1m"))
NCorr_error1 <- list()
NCorr_error2 <- list()
for (i in 1:20) {
NCorr_error1 <- append(NCorr_error1, readRDS(paste(f8, "Corr_error1_", i, ".rds", sep = "")))
NCorr_error2 <- append(NCorr_error2, readRDS(paste(f8, "Corr_error2_", i, ".rds", sep = "")))
}
Ncorr_error1 <- quantile_summary(NCorr_error1)
rownames(Ncorr_error1) <- c("C1", "C2", "P1", "P2", "P3", "P4",
"NB1", "NB2", "NB3", "NB4")
colnames(Ncorr_error1) <- rownames(Ncorr_error1)
Ncorr_error2 <- quantile_summary(NCorr_error2)
rownames(Ncorr_error2) <- rownames(Ncorr_error1)
colnames(Ncorr_error2) <- rownames(Ncorr_error2)
# display selected correlation errors
print(xtable(Ncorr_error1[c("C1", "C2", "NB1", "NB2", "P1"), c("P1", "P2")]))
print(xtable(Ncorr_error2[c("C1", "C2", "NB1", "NB2", "P1"), c("P1", "P2")],
caption = "Median (IQR) of selected correlation errors with target correlation matrix ReyN.",
label = "table_Ncorr_errors1m"))
install.packages("SimMultiCorrData")
library(SimMultiCorrData)
calc_theory("Chisq", 3)
C <- calc_theory("Chisq",3)
find_constants("Polynomial",C[3],C[4],C[5],C[6])
options(scipen=999)
find_constants("Polynomial",C[3],C[4],C[5],C[6])
library(qtl)
?scanone
citation("qtl")
?max()
data(listeria)
listeria <- calc.genoprob(listeria, step=2.5)
out <- scanone(listeria, model="2part", upper=TRUE)
# Maximum peak for LOD(p,mu)
max(out)
max(out)$pos
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
f1 <- "C:\\Users\\Allison\\Desktop\\QTLanalysis\\"
library("qtl")
library("printr")
seed <- 1234
qtd <- read.cross("csv", f1,
"recoded_GTypes2.csv", na.strings = c("NC", "FALSE", "BB"),
genotypes=c("AA", "AB"),
alleles=c("A", "B"))
summary(qtd)
epm2.em <- scanone(qtd, pheno.col = 1, model = "np", method = "em")
mar <- find.marker(qtd, chr = max(epm2.em)$chr, pos = max(epm2.em)$pos)
plotPXG(qtd, marker = mar)
effectplot(qtd, mname1 = mar)
effectplot(qtd, mname1 = paste(max(epm2.em)$chr, "@", max(epm2.em)$pos, sep = ""))
effectplot(qtd, mname1 = mar)
library(mvtnorm)
citation("mvtnorm")
