caption = "Median (IQR) of Poisson variables for example 1:
Simulations with rcorrvar and target correlation ReyN.",
label = "table_sum_pois1"))
NSum_nb2 <- list()
for (i in 1:20) {
NSum_nb2 <- append(NSum_nb2, readRDS(paste(f3, "Sum_nb2_", i, ".rds", sep = "")))
}
Nnb_qsum2 <- quantile_summary(NSum_nb2)
print(xtable(data.frame(Exp_mean = NSum_nb2[[1]][, "Exp_mean"],
mean = Nnb_qsum2[, "mean"],
Exp_var = NSum_nb2[[1]][, "Exp_var"],
var = Nnb_qsum2[, "var"],
min = Nnb_qsum2[, "min"],
max = Nnb_qsum2[, "max"]),
caption = "Median (IQR) of Negative Binomial variables for
example 1: Simulations with rcorrvar2 and target correlation
ReyN.", label = "table_sum_nb1"))
Corr_error1 <- list()
Corr_error2 <- list()
for (i in 1:20) {
Corr_error1 <- append(Corr_error1, readRDS(paste(f2, "Corr_error1_", i, ".rds", sep = "")))
Corr_error2 <- append(Corr_error2, readRDS(paste(f2, "Corr_error2_", i, ".rds", sep = "")))
}
corr_error1 <- quantile_summary(Corr_error1)
rownames(corr_error1) <- c("C1", "C2", "P1", "P2", "P3", "P4",
"NB1", "NB2", "NB3", "NB4")
colnames(corr_error1) <- rownames(corr_error1)
corr_error2 <- quantile_summary(Corr_error2)
rownames(corr_error2) <- rownames(corr_error1)
colnames(corr_error2) <- rownames(corr_error2)
# display selected correlation errors
print(xtable(corr_error1[c("C1", "C2", "NB1", "NB2", "P1"), c("P1", "P2")]))
print(xtable(corr_error2[c("C1", "C2", "NB1", "NB2", "P1"), c("P1", "P2")],
caption = "Median (IQR) of selected correlation errors with target correlation matrix Rey.",
label = "table_corr_errors1m"))
corr_errors <- matrix(1, nrow = 10, ncol = 10)
for (i in 1:nrow(corr_errors)) {
for (j in 1:ncol(corr_errors)) {
if (i == j) corr_errors[i, j] <- "0"
if (i < j) corr_errors[i, j] <- as.character(corr_error1[i, j])
if (i > j) corr_errors[i, j] <- paste("\\color{blue}",
as.character(corr_error2[i, j]),
sep = " ")
}
}
rownames(corr_errors) <- c("C1", "C2", "P1", "P2", "P3", "P4",
"NB1", "NB2", "NB3", "NB4")
colnames(corr_errors) <- rownames(corr_errors)
# display all correlation errors for table in Supplementary File
print(xtable(corr_errors[, 1:5]),
sanitize.text.function=function(x){x})
print(xtable(corr_errors[, 6:10],
caption = "Median (IQR) of correlation errors using correlation method 1
(in black) and correlation method 2 (in blue) with target correlation matrix Rey.",
label = "table_corr_errors1"),
sanitize.text.function=function(x){x})
ReyN <- read.table(paste(f3, "ReyN.txt", sep = ""))
NCorr1 <- readRDS(paste(f3, "Corr1.rds", sep = ""))
NCorr2 <- list()
for (i in 1:20) {
NCorr2 <- append(NCorr2, readRDS(paste(f3, "Corr2_", i, ".rds", sep = "")))
}
ReyN
NCorr_error1 <- list()
for (i in 1:length(NCorr1)) {
NCorr_error1[[i]] <- as.data.frame(NCorr1[[i]] - ReyN)
NCorr1[[i]] <- as.data.frame(NCorr1[[i]])
rownames(NCorr1[[i]]) <- c(1:nrow(ReyN))
rownames(NCorr_error1[[i]]) <- c(1:nrow(ReyN))
}
NCorr_error2 <- list()
for (i in 1:length(NCorr2)) {
NCorr_error2[[i]] <- as.data.frame(NCorr2[[i]] - ReyN)
NCorr2[[i]] <- as.data.frame(NCorr2[[i]])
rownames(NCorr2[[i]]) <- c(1:nrow(ReyN))
rownames(NCorr_error2[[i]]) <- c(1:nrow(ReyN))
}
Ncorr_error1 <- quantile_summary(NCorr_error1)
rownames(Ncorr_error1) <- c("C1", "C2", "P1", "P2", "P3", "P4",
"NB1", "NB2", "NB3", "NB4")
colnames(Ncorr_error1) <- rownames(Ncorr_error1)
Ncorr_error2 <- quantile_summary(NCorr_error2)
rownames(Ncorr_error2) <- rownames(Ncorr_error1)
colnames(Ncorr_error2) <- rownames(Ncorr_error2)
# display selected correlation errors
print(xtable(Ncorr_error1[c("C1", "C2", "NB1", "NB2", "P1"), c("P1", "P2")]))
print(xtable(Ncorr_error2[c("C1", "C2", "NB1", "NB2", "P1"), c("P1", "P2")],
caption = "Median (IQR) of selected correlation errors with target correlation matrix ReyN.",
label = "table_Ncorr_errors1m"))
Ncorr_errors <- matrix(1, nrow = 10, ncol = 10)
for (i in 1:nrow(Ncorr_errors)) {
for (j in 1:ncol(Ncorr_errors)) {
if (i == j) Ncorr_errors[i, j] <- "0"
if (i < j) Ncorr_errors[i, j] <- as.character(Ncorr_error1[i, j])
if (i > j) Ncorr_errors[i, j] <- paste("\\color{blue}",
as.character(Ncorr_error2[i, j]),
sep = " ")
}
}
rownames(Ncorr_errors) <- c("C1", "C2", "P1", "P2", "P3", "P4",
"NB1", "NB2", "NB3", "NB4")
colnames(Ncorr_errors) <- rownames(Ncorr_errors)
# display all correlation errors for table in Supplementary File
print(xtable(Ncorr_errors[, 1:5]),
sanitize.text.function=function(x){x})
print(xtable(Ncorr_errors[, 6:10],
caption = "Median (IQR) of correlation errors using correlation method 1
(in black) and correlation method 2 (in blue) with target correlation matrix ReyN.",
label = "table_Ncorr_errors1"),
sanitize.text.function=function(x){x})
Dist <- c("Triangular", "Nakagami")
Params <- list(c(1, 6, 4), c(1, 0.5))
# Calculate standardized cumulants
Stcum1 <- calc_theory(Dist[1], Params[[1]])
Stcum2 <- calc_theory(Dist[2], Params[[2]])
Stcum <- cbind(Stcum1, Stcum2)
colnames(Stcum) <- Dist
rownames(Stcum) <- c("mean", "sd", "skew", "skurtosis", "fifth", "sixth")
Sum_cont1 <- list()
Valid.pdf1 <- list()
Sixcorr1 <- list()
for (i in 1:20) {
Sum_cont1 <- append(Sum_cont1, readRDS(paste(f7, "Sum_cont1_", i, ".rds", sep = "")))
Valid.pdf1 <- append(Valid.pdf1, readRDS(paste(f7, "Valid.pdf1_", i, ".rds", sep = "")))
Sixcorr1 <- append(Sixcorr1, readRDS(paste(f7, "Sixcorr1_", i, ".rds", sep = "")))
}
qsum1 <- quantile_summary(Sum_cont1)
valid <- 0
false <- NULL
for (i in 1:length(Valid.pdf1)) {
valid2 <- sum(Valid.pdf1[[i]] == TRUE)
if (valid2 != 2) false <- append(false, i)
valid <- valid + valid2
}
valid
# look at values of sixth cumulant corrections needed to create valid pdf's
# and determine the most frequent sixth cumulant corrections
one <- numeric(rep)
two <- numeric(rep)
for (i in 1:rep) {
one[i] <- Sixcorr1[[i]][1]
two[i] <- Sixcorr1[[i]][2]
}
table(as.factor(one))
table(as.factor(two))
t(Stcum)
qsum1
Corr_error1 <- list()
Corr_error2 <- list()
for (i in 1:20) {
Corr_error1 <- append(Corr_error1, readRDS(paste(f7, "Corr_error1_", i, ".rds", sep = "")))
Corr_error2 <- append(Corr_error2, readRDS(paste(f7, "Corr_error2_", i, ".rds", sep = "")))
}
corr_error1 <- quantile_summary(Corr_error1)
rownames(corr_error1) <- c("C1", "C2", "P1", "P2", "P3", "P4",
"NB1", "NB2", "NB3", "NB4")
colnames(corr_error1) <- rownames(corr_error1)
corr_error2 <- quantile_summary(Corr_error2)
rownames(corr_error2) <- rownames(corr_error1)
colnames(corr_error2) <- rownames(corr_error2)
# display selected correlation errors
print(xtable(corr_error1[c("C1", "C2", "NB1", "NB2", "P1"), c("P1", "P2")]))
print(xtable(corr_error2[c("C1", "C2", "NB1", "NB2", "P1"), c("P1", "P2")],
caption = "Median (IQR) of selected correlation errors with target correlation matrix Rey.",
label = "table_corr_errors1m"))
NCorr_error1 <- list()
NCorr_error2 <- list()
for (i in 1:20) {
NCorr_error1 <- append(NCorr_error1, readRDS(paste(f8, "Corr_error1_", i, ".rds", sep = "")))
NCorr_error2 <- append(NCorr_error2, readRDS(paste(f8, "Corr_error2_", i, ".rds", sep = "")))
}
Ncorr_error1 <- quantile_summary(NCorr_error1)
rownames(Ncorr_error1) <- c("C1", "C2", "P1", "P2", "P3", "P4",
"NB1", "NB2", "NB3", "NB4")
colnames(Ncorr_error1) <- rownames(Ncorr_error1)
Ncorr_error2 <- quantile_summary(NCorr_error2)
rownames(Ncorr_error2) <- rownames(Ncorr_error1)
colnames(Ncorr_error2) <- rownames(Ncorr_error2)
# display selected correlation errors
print(xtable(Ncorr_error1[c("C1", "C2", "NB1", "NB2", "P1"), c("P1", "P2")]))
print(xtable(Ncorr_error2[c("C1", "C2", "NB1", "NB2", "P1"), c("P1", "P2")],
caption = "Median (IQR) of selected correlation errors with target correlation matrix ReyN.",
label = "table_Ncorr_errors1m"))
install.packages("SimMultiCorrData")
library(SimMultiCorrData)
calc_theory("Chisq", 3)
C <- calc_theory("Chisq",3)
find_constants("Polynomial",C[3],C[4],C[5],C[6])
options(scipen=999)
find_constants("Polynomial",C[3],C[4],C[5],C[6])
library(qtl)
?scanone
citation("qtl")
?max()
data(listeria)
listeria <- calc.genoprob(listeria, step=2.5)
out <- scanone(listeria, model="2part", upper=TRUE)
# Maximum peak for LOD(p,mu)
max(out)
max(out)$pos
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
f1 <- "C:\\Users\\Allison\\Desktop\\QTLanalysis\\"
library("qtl")
library("printr")
seed <- 1234
qtd <- read.cross("csv", f1,
"recoded_GTypes2.csv", na.strings = c("NC", "FALSE", "BB"),
genotypes=c("AA", "AB"),
alleles=c("A", "B"))
summary(qtd)
epm2.em <- scanone(qtd, pheno.col = 1, model = "np", method = "em")
mar <- find.marker(qtd, chr = max(epm2.em)$chr, pos = max(epm2.em)$pos)
plotPXG(qtd, marker = mar)
effectplot(qtd, mname1 = mar)
effectplot(qtd, mname1 = paste(max(epm2.em)$chr, "@", max(epm2.em)$pos, sep = ""))
effectplot(qtd, mname1 = mar)
library(mvtnorm)
citation("mvtnorm")
options(scipen = 999)
library("SimMultiCorrData")
library("gridExtra")
Dist <- c("Logistic", "Weibull")
Params <- list(c(0, 1), c(3, 5))
Stcum1 <- calc_theory(Dist[1], Params[[1]])
Stcum2 <- calc_theory(Dist[2], Params[[2]])
Stcum <- rbind(Stcum1, Stcum2)
rownames(Stcum) <- Dist
colnames(Stcum) <- c("mean", "sd", "skew", "skurtosis", "fifth", "sixth")
Stcum
Six <- list(seq(1.7, 1.8, 0.01), seq(0.10, 0.25, 0.01))
marginal <- list(0.3)
lam <- 0.5
pois_eps <- 0.0001
size <- 2
prob <- 0.75
nb_eps <- 0.0001
Rey <- matrix(0.8, 5, 5)
diag(Rey) <- 1
valid1 <- valid_corr(k_cat = 1, k_cont = 2, k_pois = 1, k_nb = 1,
method = "Polynomial", means = Stcum[, 1], vars = Stcum[, 2]^2,
skews = Stcum[, 3], skurts = Stcum[, 4], fifths = Stcum[, 5],
sixths = Stcum[, 6], Six = Six, marginal = marginal, lam = lam,
size = size, prob = prob, rho = Rey)
valid1
valid2 <- valid_corr2(k_cat = 1, k_cont = 2, k_pois = 1, k_nb = 1,
method = "Polynomial", means = Stcum[, 1], vars = Stcum[, 2]^2,
skews = Stcum[, 3], skurts = Stcum[, 4], fifths = Stcum[, 5],
sixths = Stcum[, 6], Six = Six, marginal = marginal, lam = lam,
pois_eps = pois_eps, size = size, prob = prob, nb_eps = nb_eps, rho = Rey)
valid2$L_rho
valid2$U_rho
Rey <- matrix(0.4, 5, 5)
diag(Rey) <- 1
valid1 <- valid_corr(k_cat = 1, k_cont = 2, k_pois = 1, k_nb = 1,
method = "Polynomial", means = Stcum[, 1], vars = Stcum[, 2]^2,
skews = Stcum[, 3], skurts = Stcum[, 4], fifths = Stcum[, 5],
sixths = Stcum[, 6], Six = Six, marginal = marginal, lam = lam,
size = size, prob = prob, rho = Rey)
# not shown in paper: check Rey using 2nd method to also get a valid result
valid2 <- valid_corr2(k_cat = 1, k_cont = 2, k_pois = 1, k_nb = 1,
method = "Polynomial", means = Stcum[, 1], vars = Stcum[, 2]^2,
skews = Stcum[, 3], skurts = Stcum[, 4], fifths = Stcum[, 5],
sixths = Stcum[, 6], Six = Six, marginal = marginal, lam = lam,
pois_eps = pois_eps, size = size, prob = prob, nb_eps = nb_eps,
rho = Rey)
logistic <- nonnormvar1(n = 10000, method = "Polynomial",
means = Stcum[1, 1], vars = Stcum[1, 2]^2, skews = Stcum[1, 3],
skurts = Stcum[1, 4], fifths = Stcum[1, 5], sixths = Stcum[1, 6],
Six = Six[[1]], seed = 33)
logistic$constants
logistic$summary_targetcont
logistic$summary_continuous
logistic$sixth_correction
logistic$valid.pdf
y_star <- qlogis(1 - 0.05, 0, 1)
y_star
f_exp <- function(z, c, y) {
return(Stcum[1, 2] * (c[1] + c[2] * z + c[3] * z^2 +
c[4] * z^3 + c[5] * z^4 + c[6] * z^5) + Stcum[1, 1] - y)
}
z_prime <- uniroot(f_exp, interval = c(-1e06, 1e06),
c = as.numeric(logistic$constants), y = y_star)$root
z_prime
1 - pnorm(z_prime)
log_cdf <- plot_sim_cdf(sim_y = logistic$continuous_variable[, 1],
calc_cprob = TRUE, delta = y_star)
log_pdf <-
plot_sim_pdf_theory(sim_y = logistic$continuous_variable[, 1],
Dist = "Logistic", params = c(0, 1))
grid.arrange(log_cdf, log_pdf, nrow = 1)
D <-
data.frame(Dist = c("Benini", "Beta", "Beta-Normal", "Birnbaum-Saunders",
"Chisq", "Dagum", "Exponential", "Exp-Geometric",
"Exp-Logarithmic", "Exp-Poisson", "F", "Fisk",
"Frechet", "Gamma", "Gaussian", "Gompertz", "Gumbel",
"Kumaraswamy", "Laplace", "Lindley", "Logistic",
"Loggamma", "Lognormal", "Lomax",
"Makeham", "Maxwell", "Nakagami", "Paralogistic",
"Pareto", "Perks", "Rayleigh", "Rice",
"Singh-Maddala", "Skewnormal", "t", "Topp-Leone",
"Triangular", "Uniform", "Weibull", "Poisson",
"Negative_Binomial"),
pdf = c("dbenini", "dbeta", "dbetanorm", "dbisa", "dchisq",
"ddagum", "dexp", "dexpgeom", "dexplog", "dexppois",
"df", "dfisk", "dfrechet", "dgamma", "dnorm",
"dgompertz", "dgumbel", "dkumar", "dlaplace",
"dlind", "dlogis", "dlgamma", "dlnorm",
"dlomax", "dmakeham", "dmaxwell", "dnaka",
"dparalogistic", "dpareto", "dperks", "dgenray",
"drice", "dsinmad", "dskewnorm", "dt", "dtopple",
"dtriangle", "dunif", "dweibull", "dpois", "dnbinom"),
fx = c("rbenini", "rbeta", "rbetanorm", "rbisa", "rchisq",
"rdagum", "rexp", "rexpgeom", "rexplog", "rexppois",
"rf", "rfisk", "rfrechet", "rgamma", "rnorm",
"rgompertz", "rgumbel", "rkumar", "rlaplace",
"rlind", "rlogis", "rlgamma", "rlnorm",
"rlomax", "rmakeham", "rmaxwell", "rnaka",
"rparalogistic", "rpareto", "rperks", "rgenray",
"rrice", "rsinmad", "rskewnorm", "rt", "rtopple",
"rtriangle", "runif", "rweibull", "rpois", "rnbinom"),
Lower = as.numeric(c(params[1], 0, -Inf, rep(0, 9),
params[1], 0, -Inf, 0, -Inf, 0, -Inf,
0, -Inf, -Inf, rep(0, 6),
params[1], rep(0, 4), -Inf, -Inf, 0,
params[1], params[1], 0, 0, 0)),
Upper = as.numeric(c(Inf, 1, rep(Inf, 15), 1, rep(Inf, 17),
1, params[2], params[2], Inf, Inf, Inf)))
calc_theory(fx = function(x) pis[1] * dnorm(x, mus[1], sigmas[1]) +
pis[2] * dnorm(x, mus[2], sigmas[2]),
lower = -Inf, upper = Inf)
library(SimMultiCorrData)
library(devtools)
?`SimMultiCorrData-package`
devtools::build_vignettes()
library(SimMultiCorrData)
C <- find_constants("Polynomial", -0.2885049, -1.1540195, 1.7930221, 6.1732675)
C <- find_constants("Polynomial", -0.2885049, -1.1540195, 1.7930221, 6.1732675, n = 50)
C <- find_constants("Polynomial", -0.2885049, -1.1540195, 1.7930221, 6.1732675, Six = seq(-50, 50, 0.01), n = 50)
library(distr)
?UnivarMixingDistribution
UnivarMixingDistribution
?sample
options(scipen = 999)
library("SimMultiCorrData")
library("gridExtra")
###########################################################################
### Section 5: Correlation matrix validation example
###########################################################################
Dist <- c("Logistic", "Weibull")
Params <- list(c(0, 1), c(3, 5))
Stcum1 <- calc_theory(Dist[1], Params[[1]])
Stcum2 <- calc_theory(Dist[2], Params[[2]])
Stcum <- rbind(Stcum1, Stcum2)
rownames(Stcum) <- Dist
colnames(Stcum) <- c("mean", "sd", "skew", "skurtosis", "fifth", "sixth")
Stcum
Six <- list(seq(1.7, 1.8, 0.01), seq(0.10, 0.25, 0.01))
marginal <- list(0.3)
lam <- 0.5
pois_eps <- 0.0001
size <- 2
prob <- 0.75
nb_eps <- 0.0001
Rey <- matrix(0.4, 5, 5)
diag(Rey) <- 1
valid1 <- valid_corr(k_cat = 1, k_cont = 2, k_pois = 1, k_nb = 1,
method = "Polynomial", means = Stcum[, 1], vars = Stcum[, 2]^2,
skews = Stcum[, 3], skurts = Stcum[, 4], fifths = Stcum[, 5],
sixths = Stcum[, 6], Six = Six, marginal = marginal, lam = lam,
size = size, prob = prob, rho = Rey)
# not shown in paper: check Rey using 2nd method to also get a valid result
valid2 <- valid_corr2(k_cat = 1, k_cont = 2, k_pois = 1, k_nb = 1,
method = "Polynomial", means = Stcum[, 1], vars = Stcum[, 2]^2,
skews = Stcum[, 3], skurts = Stcum[, 4], fifths = Stcum[, 5],
sixths = Stcum[, 6], Six = Six, marginal = marginal, lam = lam,
pois_eps = pois_eps, size = size, prob = prob, nb_eps = nb_eps,
rho = Rey)
F_lower <- calc_lower_skurt(method = "Fleishman", skews = 0,
Skurt = seq(1.1, 2, 0.01))
F_lower$Min
F_lower$Invalid.C
F_lower$SkurtCorr1
H_lower <- calc_lower_skurt(method = "Polynomial", skews = Stcum[2, 3],
fifths = Stcum[2, 5], sixths = Stcum[2, 6] + valid1$sixth_correction[2],
Skurt = seq(0.07, 1, 0.001))
H_lower$Min
H_lower$Invalid.C[which.min(H_lower$Invalid.C$skurtosis), ]
H_lower$SkurtCorr1
H_lower$Time
pis <- c(0.3, 0.4, 0.2, 0.1)
cumsum(pis)
?rcorrvar
options(scipen = 999)
seed <- 1234
n <- 10000
Dist <- c("Logistic", "Weibull")
Params <- list(c(0, 1), c(3, 5))
Stcum1 <- calc_theory(Dist[1], Params[[1]])
Stcum2 <- calc_theory(Dist[2], Params[[2]])
Stcum <- rbind(Stcum1, Stcum2)
rownames(Stcum) <- Dist
colnames(Stcum) <- c("mean", "sd", "skew", "skurtosis", "fifth", "sixth")
Stcum
Six <- list(seq(1.7, 1.8, 0.01), seq(0.10, 0.25, 0.01))
marginal <- list(0.3)
lam <- 0.5
size <- 2
prob <- 0.75
Rey <- matrix(0.4, 4, 4)
diag(Rey) <- 1
Sim1 <- rcorrvar(n = n, k_cat = 1, k_cont = 1, k_pois = 1, k_nb = 1,
method = "Polynomial", means = Stcum[1, 1],
vars = Stcum[1, 2]^2, skews = Stcum[1, 3],
skurts = Stcum[1, 4], fifths = Stcum[1, 5],
sixths = Stcum[1, 6], Six = Six[[1]], marginal = marginal,
lam = lam, size = size, prob = prob, rho = Rey,
seed = seed)
rcorrvar
names(Sim1)
Sim1$maxerr
Sim1$summary_targetcont
Sim1$summary_continuous
Sim1$sixth_correction
Sim1$valid.pdf
Sim1 <- rcorrvar(n = n, k_cat = 1, k_cont = 2, k_pois = 1, k_nb = 1,
method = "Polynomial", means = Stcum[, 1],
vars = Stcum[, 2]^2, skews = Stcum[, 3],
skurts = Stcum[, 4], fifths = Stcum[, 5],
sixths = Stcum[, 6], Six = Six, marginal = marginal,
lam = lam, size = size, prob = prob, rho = Rey,
seed = seed)
Rey <- matrix(0.4, 5, 5)
diag(Rey) <- 1
Sim1 <- rcorrvar(n = n, k_cat = 1, k_cont = 2, k_pois = 1, k_nb = 1,
method = "Polynomial", means = Stcum[, 1],
vars = Stcum[, 2]^2, skews = Stcum[, 3],
skurts = Stcum[, 4], fifths = Stcum[, 5],
sixths = Stcum[, 6], Six = Six, marginal = marginal,
lam = lam, size = size, prob = prob, rho = Rey,
seed = seed)
Sim1$valid.pdf
Sim1$sixth_correction
Six[[1]]
Rey <- matrix(0.4, 4, 4)
diag(Rey) <- 1
Rey <- matrix(0.4, 4, 4)
diag(Rey) <- 1
# Make sure Rey is within upper and lower correlation limits
valid <- valid_corr(k_cat = 1, k_cont = 1, k_pois = 1, k_nb = 1,
method = "Polynomial", means = Stcum[1, 1],
vars = Stcum[1, 2]^2, skews = Stcum[1, 3],
skurts = Stcum[1, 4], fifths = Stcum[1, 5],
sixths = Stcum[1, 6], Six = list(Six[[1]]), marginal = marginal,
lam = lam, size = size, prob = prob, rho = Rey,
seed = seed)
# Simulate variables without error loop
Sim1 <- rcorrvar(n = n, k_cat = 1, k_cont = 1, k_pois = 1, k_nb = 1,
method = "Polynomial", means = Stcum[1, 1],
vars = Stcum[1, 2]^2, skews = Stcum[1, 3],
skurts = Stcum[1, 4], fifths = Stcum[1, 5],
sixths = Stcum[1, 6], Six = list(Six[[1]]), marginal = marginal,
lam = lam, size = size, prob = prob, rho = Rey,
seed = seed)
Sim1$valid.pdf
Sim1$sixth_correction
options(scipen = 999)
seed <- 1234
n <- 10000
Dist <- c("Logistic", "Weibull")
Params <- list(c(0, 1), c(3, 5))
Stcum1 <- calc_theory(Dist[1], Params[[1]])
Stcum2 <- calc_theory(Dist[2], Params[[2]])
Stcum <- rbind(Stcum1, Stcum2)
rownames(Stcum) <- Dist
colnames(Stcum) <- c("mean", "sd", "skew", "skurtosis", "fifth", "sixth")
Stcum
Six <- list(seq(1.7, 1.8, 0.01), seq(0.10, 0.25, 0.01))
marginal <- list(0.3)
lam <- 0.5
size <- 2
prob <- 0.75
Rey <- matrix(0.4, 5, 5)
diag(Rey) <- 1
Sim1 <- rcorrvar(n = n, k_cat = 1, k_cont = 2, k_pois = 1, k_nb = 1,
method = "Fleishman", means = Stcum[, 1],
vars = Stcum[, 2]^2, skews = Stcum[, 3],
skurts = Stcum[, 4], marginal = marginal,
lam = lam, size = size, prob = prob, rho = Rey,
seed = seed)
valid <- valid_corr(k_cat = 1, k_cont = 2, k_pois = 1, k_nb = 1,
method = "Polynomial", means = Stcum[, 1],
vars = Stcum[, 2]^2, skews = Stcum[, 3],
skurts = Stcum[, 4], marginal = marginal,
lam = lam, size = size, prob = prob, rho = Rey,
seed = seed)
valid <- valid_corr(k_cat = 1, k_cont = 2, k_pois = 1, k_nb = 1,
method = "Fleishman", means = Stcum[, 1],
vars = Stcum[, 2]^2, skews = Stcum[, 3],
skurts = Stcum[, 4], marginal = marginal,
lam = lam, size = size, prob = prob, rho = Rey,
seed = seed)
library(SimMultiCorrData)
Sim1 <- rcorrvar(n = n, k_cat = 1, k_cont = 2, k_pois = 1, k_nb = 1,
method = "Fleishman", means = Stcum[, 1],
vars = Stcum[, 2]^2, skews = Stcum[, 3],
skurts = Stcum[, 4], marginal = marginal,
lam = lam, size = size, prob = prob, rho = Rey,
seed = seed)
library(SimMultiCorrData)
names(Sim1)
Sim1$summary_continuous
system("R CMD Rd2pdf C:\\Users\\Allison\\Documents\\SimMultiCorrData")
