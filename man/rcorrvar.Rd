% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/rcorrvar.R
\name{rcorrvar}
\alias{rcorrvar}
\title{Generation of Correlated Ordinal, Continuous, Poisson, and/or Negative Binomial Variables: Method 1}
\usage{
rcorrvar(n = 10000, k_cont = 0, k_cat = 0, k_pois = 0, k_nb = 0,
  method = c("Fleishman", "Polynomial"), means, vars, skews, skurts, fifths,
  sixths, Six = list(), marginal = list(), support = list(),
  nrand = 1e+05, lam = NULL, size = NULL, prob = NULL, mu = NULL,
  Sigma = NULL, rho = NULL, cstart = NULL, seed = 1234,
  errorloop = FALSE, epsilon = 0.001, maxit = 1000,
  extra_correct = TRUE)
}
\arguments{
\item{n}{the sample size (i.e. the length of each simulated variable; default = 10000)}

\item{k_cont}{the number of continuous variables (default = 0)}

\item{k_cat}{the number of ordinal (r >= 2 categories) variables (default = 0)}

\item{k_pois}{the number of Poisson variables (default = 0)}

\item{k_nb}{the number of Negative Binomial variables (default = 0)}

\item{method}{the method used to generate the k_cont continuous variables.  "Fleishman" uses a third-order polynomial transformation
and "Polynomial" uses Headrick's fifth-order transformation.}

\item{means}{a vector of means for the k_cont continuous variables (i.e. = rep(0, k_cont))}

\item{vars}{a vector of variances (i.e. = rep(1, k_cont))}

\item{skews}{a vector of skewness values (i.e. = rep(0, k_cont))}

\item{skurts}{a vector of standardized kurtoses (kurtosis - 3, so that normal variables have a value of 0; i.e. = rep(0, k_cont))}

\item{fifths}{a vector of standardized fifth cumulants (not necessary for \code{method} = "Fleishman"; i.e. = rep(0, k_cont))}

\item{sixths}{a vector of standardized sixth cumulants (not necessary for \code{method} = "Fleishman"; i.e. = rep(0, k_cont))}

\item{Six}{a list of vectors of correction values to add to the sixth cumulants if no valid pdf constants are found,
ex: \code{Six = list(seq(0.01, 2,by = 0.01), seq(1, 10,by = 0.5))}; if no correction is desired for variable Y_i, set set the i-th list
component equal to NULL}

\item{marginal}{a list of length equal to \code{k_cat}; the i-th element is a vector of the cumulative
probabilities defining the marginal distribution of the i-th variable;
if the variable can take r values, the vector will contain r - 1 probabilities (the r-th is assumed to be 1; default = list());
for binary variables, these should be input the same as for ordinal variables with more than 2 categories (i.e. the user-specified
probability is the probability of the 1st category, which has the smaller support value)}

\item{support}{a list of length equal to \code{k_cat}; the i-th element is a vector containing the r ordered support values;
if not provided (i.e. \code{support = list()}), the default is for the i-th element to be the
vector 1, ..., r}

\item{nrand}{the number of random numbers to generate in calculating intermediate correlations (default = 10000)}

\item{lam}{a vector of lambda (> 0) constants for the Poisson variables (see \code{\link[stats]{dpois}})}

\item{size}{a vector of size parameters for the Negative Binomial variables (see \code{\link[stats]{dnbinom}})}

\item{prob}{a vector of success probability parameters}

\item{mu}{a vector of mean parameters (*Note: either \code{prob} or \code{mu} should be supplied for all Negative Binomial variables,
not a mixture; default = NULL)}

\item{Sigma}{an intermediate correlation matrix to use if the user wants to provide one (default = NULL)}

\item{rho}{the target correlation matrix (\emph{must be ordered ordinal, continuous, Poisson, Negative Binomial}; default = NULL)}

\item{cstart}{a list containing initial values for root-solving algorithm used in \code{\link[SimMultiCorrData]{find_constants}}
(see \code{\link[BB]{multiStart}} for \code{method} = "Fleishman" or \code{\link[nleqslv]{nleqslv}} for \code{method} = "Polynomial").
If user specified, each list element must be input as a matrix. If no starting values are specified for a given continuous
variable, that list element should be NULL.  If NULL and all 4 standardized cumulants (rounded to 3 digits) are within
0.01 of those in Headrick's common distribution table (see \code{\link[SimMultiCorrData]{Headrick.dist}}
data), uses his constants as starting values; else, generates n sets of random starting values from
uniform distributions.}

\item{seed}{the seed value for random number generation (default = 1234)}

\item{errorloop}{if TRUE, uses \code{\link[SimMultiCorrData]{error_loop}} to attempt to correct the final correlation
(default = FALSE)}

\item{epsilon}{the maximum acceptable error between the final and target correlation matrices (default = 0.001)
in the calculation of ordinal intermediate correlations with \code{\link[SimMultiCorrData]{ordnorm}} or in the error loop}

\item{maxit}{the maximum number of iterations to use (default = 1000) in the calculation of ordinal
intermediate correlations with \code{\link[SimMultiCorrData]{ordnorm}} or in the error loop}

\item{extra_correct}{if TRUE, within each variable pair, if the maximum correlation error is still greater than 0.1, the intermediate
correlation is set equal to the target correlation (with the assumption that the calculated final correlation will be
less than 0.1 away from the target)}
}
\value{
A list whose components vary based on the type of simulated variables.  Simulated variables are returned as data.frames:

If \bold{ordinal variables} are produced:

    \code{ordinal_variables} the generated ordinal variables,

    \code{summary_categorical} a list, where the i-th element contains a cumulative probability table for ordinal variable Y_i

If \bold{continuous variables} are produced:

    \code{constants} a data.frame of the constants,

    \code{continuous_variables} the generated continuous variables,

    \code{summary_continuous} a data.frame containing a summary of each variable,

    \code{sixth_correction} a vector of sixth cumulant correction values,

    \code{valid.pdf} a vector where the i-th element is "TRUE" if the constants for the i-th continuous variable generate a valid pdf,
                else "FALSE"

If \bold{Poisson variables} are produced:

    \code{Poisson_variables} the generated Poisson variables,

    \code{summary_Poisson} a data.frame containing a summary of each variable

If \bold{Negative Binomial variables} are produced:

    \code{Neg_Bin_variables} the generated Negative Binomial variables,

    \code{summary_Neg_Bin} a data.frame containing a summary of each variable

Additionally, the following elements:

    \code{correlations} the final correlation matrix,

    \code{Sigma1} the intermediate correlation before the error loop,

    \code{Sigma2} the intermediate correlation matrix after the error loop,

    \code{Constants_Time} the time in minutes required to calculate the constants,

    \code{Intercorrelation_Time} the time in minutes required to calculate the intermediate correlation matrix,

    \code{Error_Loop_Time} the time in minutes required to use the error loop,

    \code{Simulation_Time} the total simulation time in minutes,

    \code{niter} a matrix of the number of iterations used for each variable in the error loop,

    \code{maxerr} the maximum final correlation error (from the target rho).

    If a particular element is not required, the result is NULL for that element.
}
\description{
This function simulates \code{k_cat} ordinal, \code{k_cont} continuous, \code{k_pois} Poisson, and/or \code{k_nb}
    Negative Binomial variables with
    a specified correlation matrix \code{rho}.  The variables are generated from multivariate normal variables with intermediate correlation
    matrix \code{Sigma}, calculated by \code{\link[SimMultiCorrData]{findintercorr}}, and then transformed.  The \emph{ordering} of the
    variables in \code{rho} must be \emph{ordinal} (r >= 2 categories), \emph{continuous}, \emph{Poisson}, and \emph{Negative Binomial}
    (note that it is possible for \code{k_cat}, \code{k_cont}, \code{k_pois}, and/or \code{k_nb} to be 0).
}
\section{Overview of Method 1}{

    The intermediate correlations used in method 1 are more simulation based than those in method 2, which means that accuracy
    increases with sample size and the number of repetitions.  In addition, specifying the seed allows for reproducibility.  In
    addition, method 1 differs from method 2 in the following ways:

    1) The intermediate correlation for \bold{count variables} is based on the method of Yahav & Shmueli (2012), which uses a
    simulation based, logarithmic transformation of the target correlation.  This method becomes less accurate as the variable mean
    gets closer to zero.

    2) The \bold{ordinal - count variable} correlations are based on an extension of the method of Amatya & Demirtas (2015), in which
    the correlation correction factor is the product of the upper Frechet-Hoeffding bound on the correlation between the count
    variable and the normal variable used to generate it and a simulated upper bound on the correlation between an ordinal variable
    and the normal variable used to generate it (see Demirtas & Hedeker, 2011).

    3) The \bold{continuous - count variable} correlations are based on an extension of the methods of Amatya & Demirtas (2015) and
    Demirtas et al. (2012), in which the correlation correction factor is the product of the upper Frechet-Hoeffding bound
    on the correlation between the count variable and the normal variable used to generate it and the power method correlation
    between the continuous variable and the normal variable used to generate it (see Headrick & Kowalchuk, 2007).  The
    intermediate correlations are the ratio of the target correlations to the correction factor.

    Please see the \bold{Comparison of Method 1 and Method 2} vignette for more information and an step-by-step overview of the
    simulation process.
}

\section{Reasons for Function Errors}{

    The most likely cause for function errors is that no solutions to \code{\link[SimMultiCorrData]{fleish}} or
    \code{\link[SimMultiCorrData]{poly}} converged when using \code{\link[SimMultiCorrData]{find_constants}}.  If this happens,
    the simulation will stop.  It may help to first use \code{\link[SimMultiCorrData]{find_constants}} for each continuous variable to
    determine if a vector of sixth cumulant correction values is needed.  The solutions can be used as starting values (see \code{cstart} below).
    In addition, the kurtosis may be outside the region of possible values.  There is an associated lower boundary for kurtosis associated
    with a given skew (for Fleishman's method) or skew and fifth and sixth cumulants (for Headrick's method).  Use
    \code{\link[SimMultiCorrData]{calc_lower_skurt}} to determine the boundary for a given set of cumulants.

    In addition, as mentioned above, the feasibility of the final correlation matrix rho, given the
    distribution parameters, should be checked first using \code{\link[SimMultiCorrData]{valid_corr}}.  This function either checks
    if a given \code{rho} is plausible or returns the lower and upper final correlation limits.  It should be noted that even if a target
    correlation matrix is within the "plausible range," it still may not be possible to achieve the desired matrix.  This happens most
    frequently when generating ordinal variables (r >= 2 categories).  The error loop frequently fixes these problems.
}

\examples{
\dontrun{

# Binary, Ordinal, Continuous, Poisson, and Negative Binomial Variables

options(scipen = 999)
seed <- 1234
n <- 10000

# Continuous Distributions: Normal, t (df = 10), Chisq (df = 4),
#                           Beta (a = 4, b = 2), Gamma (a = 4, b = 4)
Dist <- c("Gaussian", "t", "Chisq", "Beta", "Gamma")

# calculate standardized cumulants

M1 <- calc_theory(Dist = "Gaussian", params = c(0, 1))
M2 <- calc_theory(Dist = "t", params = 10)
M3 <- calc_theory(Dist = "Chisq", params = 4)
M4 <- calc_theory(Dist = "Beta", params = c(4, 2))
M5 <- calc_theory(Dist = "Gamma", params = c(4, 4))
M <- cbind(M1, M2, M3, M4, M5)
M <- round(M[-c(1:2),], digits = 6)
colnames(M) <- Dist
rownames(M) <- c("skew", "skurtosis", "fifth", "sixth")
means <- rep(0, length(Dist))
vars <- rep(1, length(Dist))

# Binary and Ordinal Distributions
marginal <- list(0.3, 0.4, c(0.1, 0.5), c(0.3, 0.6, 0.9),
                 c(0.2, 0.4, 0.7, 0.8))
support <- list()

# Poisson Distributions
lam <- c(1, 5, 10)

# Negative Binomial Distributions
size <- c(3, 6)
prob <- c(0.2, 0.8)

ncat <- length(marginal)
ncont <- ncol(M)
npois <- length(lam)
nnb <- length(size)

# Create correlation matrix from a uniform distribution (-0.8, 0.8)
set.seed(seed)
Rey <- diag(1, nrow = (ncat + ncont + npois + nnb))
for (i in 1:nrow(Rey)) {
  for (j in 1:ncol(Rey)) {
    if (i > j) Rey[i, j] <- runif(1, -0.8, 0.8)
    Rey[j, i] <- Rey[i, j]
  }
}

# Test for positive-definiteness
library(Matrix)
if(min(eigen(Rey, symmetric = TRUE)$values) < 0) {
  Rey <- as.matrix(nearPD(Rey, corr = T, keepDiag = T)$mat)
}

# Make sure Rey is within upper and lower correlation limits
valid <- valid_corr(k_cat = ncat, k_cont = ncont, k_pois = npois,
                    k_nb = nnb, method = "Polynomial", means = means,
                    vars = vars, skews = M[1, ], skurts = M[2, ],
                    fifths = M[3, ], sixths = M[4, ], Six = NULL,
                    marginal = marginal, lam = lam, size = size,
                    prob = prob, mu = NULL, rho = Rey, n = 100000,
                    seed = seed)

# Simulate variables without error loop
A <- rcorrvar(n = 10000, k_cont = ncont, k_cat = ncat, k_pois = npois,
              k_nb = nnb, method = "Polynomial", means = means, vars = vars,
              skews = M[1, ], skurts = M[2, ], fifths = M[3, ],
              sixths = M[4, ], Six = NULL, marginal = marginal,
              support = list(), nrand = 100000,
              lam = lam, size = size, prob = prob, mu = NULL,
              Sigma = NULL, rho = Rey, cstart = NULL, seed = seed,
              errorloop = FALSE, epsilon = 0.001, maxit = 1000,
              extra_correct = TRUE)

A$maxerr
Acorr_error = round(A$correlations - Rey, 6)
Acorr_error

# interquartile-range of correlation errors
quantile(as.numeric(Acorr_error), 0.25)
quantile(as.numeric(Acorr_error), 0.75)

# Simulate variables with error loop
B <- rcorrvar(n = 10000, k_cont = ncont, k_cat = ncat, k_pois = npois,
              k_nb = nnb, method = "Polynomial", means = means, vars = vars,
              skews = M[1, ], skurts = M[2, ], fifths = M[3, ],
              sixths = M[4, ], Six = NULL, marginal = marginal,
              support = list(), nrand = 100000,
              lam = lam, size = size, prob = prob, mu = NULL,
              Sigma = NULL, rho = Rey, cstart = NULL, seed = seed,
              errorloop = TRUE, epsilon = 0.001, maxit = 1000,
              extra_correct = TRUE)

B$maxerr
Bcorr_error = round(B$correlations - Rey, 6)
Bcorr_error

# interquartile-range of correlation errors
quantile(as.numeric(Bcorr_error), 0.25)
quantile(as.numeric(Bcorr_error), 0.75)

# Look at results
names(B)

# Ordinal variables
marginal
B$summary_categorical

# Continuous variables
round(B$constants, 6)
t(M)
round(B$summary_continuous, 6)
B$valid.pdf

# Count variables
B$summary_Poisson
B$summary_Neg_Bin

# Generate Plots

# t (df = 10) (2nd continuous variable)
# 1) Simulated Data CDF (find cumulative probability up to y = 0.5)
plot_sim_cdf(B$continuous_variables[, 2], delta = 0.5)

# 2) Simulated Data and Target Distribution PDFs
plot_sim_pdf_theory(B$continuous_variables[, 2], Dist = "t", params = 10)

# 3) Simulated Data and Target Distribution
plot_sim_theory(B$continuous_variables[, 2], Dist = "t", params = 10)

# Chisq (df = 4) (3rd continuous variable)
# 1) Simulated Data CDF (find cumulative probability up to y = 0.5)
plot_sim_cdf(B$continuous_variables[, 3], delta = 0.5)

# 2) Simulated Data and Target Distribution PDFs
plot_sim_pdf_theory(B$continuous_variables[, 3], Dist = "Chisq", params = 4)

# 3) Simulated Data and Target Distribution
plot_sim_theory(B$continuous_variables[, 3], Dist = "Chisq", params = 4)

# Compare to Method 2
# Make sure Rey is within upper and lower correlation limits
valid2 <- valid_corr2(k_cat = ncat, k_cont = ncont, k_pois = npois,
                     k_nb = nnb, method = "Polynomial", means = means,
                     vars = vars, skews = M[1, ], skurts = M[2, ],
                     fifths = M[3, ], sixths = M[4, ], Six = NULL,
                     marginal = marginal, lam = lam,
                     pois_eps = rep(0.0001, npois),
                     size = size, prob = prob, mu = NULL,
                     nb_eps = rep(0.0001, nnb),
                     rho = Rey, n = 100000, seed = seed)

# Simulate variables without error loop
C <- rcorrvar2(n = 10000, k_cont = ncont, k_cat = ncat, k_pois = npois,
               k_nb = nnb, method = "Polynomial", means = means,
               vars = vars, skews = M[1, ], skurts = M[2, ],
               fifths = M[3, ], sixths = M[4, ], Six = NULL,
               marginal = marginal, support = list(),
               lam = lam, pois_eps = rep(0.0001, npois),
               size = size, prob = prob, mu = NULL,
               nb_eps = rep(0.0001, nnb),
               Sigma = NULL, rho = Rey, cstart = NULL, seed = seed,
               errorloop = FALSE, epsilon = 0.001, maxit = 1000,
               extra_correct = TRUE)

C$maxerr
Ccorr_error = round(C$correlations - Rey, 6)
Ccorr_error

# interquartile-range of correlation errors
quantile(as.numeric(Ccorr_error), 0.25)
quantile(as.numeric(Ccorr_error), 0.75)

# Simulate variables with error loop
D <- rcorrvar2(n = 10000, k_cont = ncont, k_cat = ncat, k_pois = npois,
               k_nb = nnb, method = "Polynomial", means = means,
               vars = vars, skews = M[1, ], skurts = M[2, ],
               fifths = M[3, ], sixths = M[4, ], Six = NULL,
               marginal = marginal, support = list(),
               lam = lam, pois_eps = rep(0.0001, npois),
               size = size, prob = prob, mu = NULL,
               nb_eps = rep(0.0001, nnb),
               Sigma = NULL, rho = Rey, cstart = NULL, seed = seed,
               errorloop = TRUE, epsilon = 0.001, maxit = 1000,
               extra_correct = TRUE)

D$maxerr
Dcorr_error = round(D$correlations - Rey, 6)
Dcorr_error

# interquartile-range of correlation errors
quantile(as.numeric(Dcorr_error), 0.25)
quantile(as.numeric(Dcorr_error), 0.75)
}
}
\references{
Ferrari PA, Barbiero A (2012). Simulating ordinal data, Multivariate Behavioral Research, 47(4): 566-589.

Barbiero A, Ferrari PA (2015). GenOrd: Simulation of Discrete Random Variables with Given
Correlation Matrix and Marginal Distributions. R package version 1.4.0. \url{https://CRAN.R-project.org/package=GenOrd}

Higham N (2002). Computing the nearest correlation matrix - a problem from finance; IMA Journal of Numerical Analysis 22: 329-343.

Olsson U, Drasgow F, & Dorans NJ (1982). The Polyserial Correlation Coefficient. Psychometrika, 47(3): 337-47.
    \doi{10.1007/BF02294164}.

Headrick TC (2002). Fast Fifth-order Polynomial Transforms for Generating Univariate and Multivariate
    Non-normal Distributions. Computational Statistics & Data Analysis 40(4):685-711
    (\href{http://www.sciencedirect.com/science/article/pii/S0167947302000725}{ScienceDirect})

Fleishman AI (1978). A Method for Simulating Non-normal Distributions. Psychometrika, 43, 521-532.

Headrick TC, Sawilowsky SS (1999). Simulating Correlated Non-normal Distributions: Extending the Fleishman Power
    Method. Psychometrika, 64, 25-35.

Headrick TC, Kowalchuk RK (2007). The Power Method Transformation: Its Probability Density Function, Distribution
    Function, and Its Further Use for Fitting Data. Journal of Statistical Computation and Simulation, 77, 229-249.

Headrick TC, Sheng Y, & Hodis FA (2007). Numerical Computing and Graphics for the Power Method Transformation Using
    Mathematica. Journal of Statistical Software, 19(3), 1 - 17. \doi{10.18637/jss.v019.i03}.

Headrick TC (2004). On Polynomial Transformations for Simulating Multivariate Nonnormal Distributions.
    Journal of Modern Applied Statistical Methods, 3, 65-71.

Demirtas H, Hedeker D, & Mermelstein RJ (2012). Simulation of massive public health data by power polynomials.
    Statistics in Medicine 31:27, 3337-3346.

Amatya A & Demirtas H (2015). Simultaneous generation of multivariate mixed data with Poisson and normal marginals.
    Journal of Statistical Computation and Simulation, 85(15): 3129-39.

Amatya A & Demirtas H (2016). PoisNor: Simultaneous Generation of Multivariate Data with Poisson and Normal Marginals.
    R package version 1.1. \url{https://CRAN.R-project.org/package=PoisNor}

Yahav I & Shmueli G (2012). On Generating Multivariate Poisson Data in Management Science Applications. Applied Stochastic
    Models in Business and Industry, 28(1): 91-102. \doi{10.1002/asmb.901}.

Demirtas H & Hedeker D (2011). A practical way for computing approximate lower and upper correlation bounds.
    American Statistician, 65(2): 104-109.

Hoeffding W. Scale-invariant correlation theory. In: Fisher NI, Sen PK, editors. The collected works of Wassily Hoeffding.
    New York: Springer-Verlag; 1994. p. 57-107.

Frechet M.  Sur les tableaux de correlation dont les marges sont donnees.  Ann. l'Univ. Lyon SectA.  1951;14:53-77.

Varadhan R, Gilbert PD (2009). BB: An R Package for Solving a Large System of Nonlinear Equations and for
    Optimizing a High-Dimensional Nonlinear Objective Function, J. Statistical Software, 32:4,
    \url{http://www.jstatsoft.org/v32/i04/}

Berend Hasselman (2017). nleqslv: Solve Systems of Nonlinear Equations. R package version 3.2.
    \url{https://CRAN.R-project.org/package=nleqslv}
}
\seealso{
\code{\link[SimMultiCorrData]{find_constants}}, \code{\link[SimMultiCorrData]{findintercorr}},
    \code{\link[BB]{multiStart}}, \code{\link[nleqslv]{nleqslv}}
}
\keyword{Binomial,}
\keyword{Fleishman,}
\keyword{Headrick,}
\keyword{Negative}
\keyword{Poisson,}
\keyword{continuous,}
\keyword{method1}
\keyword{ordinal,}
\keyword{simulation,}
